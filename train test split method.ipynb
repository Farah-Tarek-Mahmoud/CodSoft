{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# from category_encoders import TargetEncoder\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Prepare features\n",
    "# X = df_filtered.drop(columns=['Rating'])\n",
    "# y = df_filtered['Rating']\n",
    "\n",
    "# # Split into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# # Apply K-Fold cross-validation for target encoding\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# # Initialize the TargetEncoder\n",
    "# encoder = TargetEncoder(cols=['Genre', 'Director', 'Name', 'Actor 1', 'Actor 2', 'Actor 3'])\n",
    "\n",
    "# # Apply the encoder to the training data using cross-validation\n",
    "# for train_index, val_index in kf.split(X_train):\n",
    "#     X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "#     y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "#     # Fit encoder on the training fold\n",
    "#     encoder.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "#     # Transform both the training fold and validation fold\n",
    "#     X_train.loc[val_index, ['Genre_encoded', 'Director_encoded', 'Name_encoded', 'Actor1_encoded', 'Actor2_encoded', 'Actor3_encoded']] = encoder.transform(X_val_fold)\n",
    "\n",
    "# print(X_train.index)\n",
    "# X_train = X_train.reset_index(drop=True)\n",
    "\n",
    "# # Fit on the entire train set and transform test set (no data leakage)\n",
    "# encoder.fit(X_train, y_train)\n",
    "# X_test[['Genre_encoded', 'Director_encoded', 'Name_encoded', 'Actor1_encoded', 'Actor2_encoded', 'Actor3_encoded']] = encoder.transform(X_test)\n",
    "\n",
    "# # Drop original categorical columns from X_train and X_test\n",
    "# X_train = X_train.drop(columns=['Genre', 'Director', 'Name', 'Actor 1', 'Actor 2', 'Actor 3'])\n",
    "# X_test = X_test.drop(columns=['Genre', 'Director', 'Name', 'Actor 1', 'Actor 2', 'Actor 3'])\n",
    "\n",
    "# # Standardize the data\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Now you can fit your model (Lasso or Random Forest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split the dataset into train and test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_filtered, df_filtered['Rating'], test_size=0.2, random_state=42)\n",
    "\n",
    "# # encode the categorical features\n",
    "# for col in ['Genre', 'Director', 'Name', 'Actor 1', 'Actor 2', 'Actor 3']:\n",
    "#     mean_encoded = X_train.groupby(col)['Rating'].transform('mean')\n",
    "#     X_train[col+'_encoded'] = mean_encoded\n",
    "#     # Apply the same transformation to the test set\n",
    "#     X_test[col+'_encoded'] = X_test[col].map(mean_encoded.to_dict()).fillna(X_train['Genre_encoded'].mean())\n",
    "\n",
    "# # Drop the original categorical columns (the ones that contain strings)\n",
    "# X_train = X_train.drop(columns=['Genre', 'Director', 'Name', 'Actor 1', 'Actor 2', 'Actor 3', \"Rating\"])\n",
    "# X_test = X_test.drop(columns=['Genre', 'Director', 'Name', 'Actor 1', 'Actor 2', 'Actor 3', \"Rating\"])\n",
    "\n",
    "# # print the shape of the train and test sets\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)\n",
    "\n",
    "# print(X_train.head())\n",
    "# print(X_train.isna().sum())\n",
    "# X_train.fillna(X_train.mean(), inplace=True)\n",
    "# print(X_test.isna().sum())\n",
    "# print(X_train.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_ratings = pd.read_csv(\"IMDB Movies India.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# # display shape of the dataset\n",
    "print(f\"the shape of th df: {df_ratings.shape}\")\n",
    "\n",
    "# # display top 5 rows of the dataset to get an idea\n",
    "print(\"\\n\", df_ratings.head())\n",
    "\n",
    "# print information about the dataset\n",
    "print(\"\\n\",df_ratings.info())\n",
    "\n",
    "# print statistical description of the dataset\n",
    "print(df_ratings.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the dataset\n",
    "df_filtered = df_ratings.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "# extract the year from the 'Year' column\n",
    "df_filtered['Year'] = df_filtered['Year'].str.extract(r\"(\\d+)\")\n",
    "\n",
    "# convert the 'Year' column to integer\n",
    "df_filtered['Year'] = df_filtered['Year'].astype('int64')\n",
    "\n",
    "print(df_filtered['Year'].unique())\n",
    "print(df_filtered['Year'].dtypes)\n",
    "\n",
    "# extract the duration in numbers from the \"Duration\" column\n",
    "df_filtered['Duration'] = df_filtered['Duration'].str.extract(r\"(\\d+)\")\n",
    "# convert the 'Duration' column to integer\n",
    "df_filtered['Duration'] = df_filtered['Duration'].astype('int64')\n",
    "# print to check the result\n",
    "print(df_filtered['Duration'].unique())\n",
    "print(df_filtered['Duration'].dtypes)\n",
    "# extract the True letters(Text) from the \"Name\" column\n",
    "df_filtered['Name'] = df_filtered['Name'].str.extract('([A-Za-z\\s\\'\\-]+)')\n",
    "\n",
    "# print to check the result\n",
    "print(df_filtered['Name'].unique())\n",
    "# replace the , to keep only numerical part\n",
    "df_filtered['Votes'] = df_filtered['Votes'].str.replace(',', '')\n",
    "\n",
    "# convert the 'Votes' column to integer\n",
    "df_filtered['Votes'] = df_filtered['Votes'].astype('int64')\n",
    "# print to check the result\n",
    "print(df_filtered['Votes'].unique())\n",
    "print(df_filtered['Votes'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_ratings = pd.read_csv(\"IMDB Movies India.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# # display shape of the dataset\n",
    "print(df_ratings.shape)\n",
    "\n",
    "# # display top 5 rows of the dataset to get an idea\n",
    "print(df_ratings.head())\n",
    "\n",
    "# print information about the dataset\n",
    "print(df_ratings.info())\n",
    "\n",
    "# print statistical description of the dataset\n",
    "print(df_ratings.describe())\n",
    "# visualize missing values in the dataset\n",
    "sns.heatmap(df_ratings.isnull(), cbar=False)\n",
    "# print number of missing values in each column\n",
    "print(df_ratings.isnull().sum())\n",
    "\n",
    "\n",
    "# calculate the threshold for missing values\n",
    "threshold_Na = len(df_ratings) * 0.05\n",
    "print(f\"threshold_Na: {threshold_Na}\")\n",
    "#Checking the unique values in each column\n",
    "for col in df_ratings.select_dtypes(include = \"object\"):\n",
    "    print(f\"The unique values in Column: {col}\")\n",
    "    print(df_ratings[col].unique())\n",
    "#Checking the values in each column\n",
    "for col in df_ratings.select_dtypes(include = \"object\"):\n",
    "    print(f\"the values in Column: {col}\")\n",
    "    print(df_ratings[col].value_counts(), \"\\n\\n\")\n",
    "# make a copy of the dataset\n",
    "df_filltered = df_ratings.copy()\n",
    "\n",
    "print(df_filltered.shape)\n",
    "\n",
    "# drop columns with missing values more than threshold\n",
    "df_filltered.dropna(inplace=True)\n",
    "\n",
    "# display the information about the dataset\n",
    "print(df_filltered.info())\n",
    "\n",
    "# check missing values\n",
    "print(df_filltered.isnull().sum())\n",
    "# extract the year from the 'Year' column\n",
    "df_filltered['Year'] = df_filltered['Year'].str.extract(r\"(\\d+)\")\n",
    "\n",
    "# convert the 'Year' column to integer\n",
    "df_filltered['Year'] = df_filltered['Year'].astype('int64')\n",
    "\n",
    "print(df_filltered['Year'].unique())\n",
    "print(df_filltered['Year'].dtypes)\n",
    "# extract the duration in numbers from the \"Duration\" column\n",
    "df_filltered['Duration'] = df_filltered['Duration'].str.extract(r\"(\\d+)\")\n",
    "# convert the 'Duration' column to integer\n",
    "df_filltered['Duration'] = df_filltered['Duration'].astype('int64')\n",
    "# print to check the result\n",
    "print(df_filltered['Duration'].unique())\n",
    "print(df_filltered['Duration'].dtypes)\n",
    "# extract the True letters(Text) from the \"Name\" column\n",
    "df_filltered['Name'] = df_filltered['Name'].str.extract('([A-Za-z\\s\\'\\-]+)')\n",
    "\n",
    "# print to check the result\n",
    "print(df_filltered['Name'].unique())\n",
    "# replace the , to keep only numerical part\n",
    "df_filltered['Votes'] = df_filltered['Votes'].str.replace(',', '')\n",
    "\n",
    "# convert the 'Votes' column to integer\n",
    "df_filltered['Votes'] = df_filltered['Votes'].astype('int64')\n",
    "# print to check the result\n",
    "print(df_filltered['Votes'].unique())\n",
    "print(df_filltered['Votes'].dtypes)\n",
    "#checking duplicate values by Name and Year\n",
    "duplicated_vals = df_filltered.groupby(['Name', 'Year']).filter(lambda x: len(x) > 1)\n",
    "duplicated_vals.head()\n",
    "# drop the duplicate values\n",
    "df_filltered.drop_duplicates(subset=['Name', 'Year'], keep='last', inplace=True)\n",
    "print(df_filltered.shape)\n",
    "# exploring the dataset further\n",
    "print(df_filltered.head())\n",
    "print(df_filltered.describe())\n",
    "# visiualize distribution of the 'Rating' column\n",
    "sns.histplot(df_filltered['Rating'])\n",
    "plt.title(\"Distribution of Rating\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Count\")\n",
    "_ = plt.xticks(np.arange(0, 11, 1))\n",
    "# the rating across the years\n",
    "rating_year = df_filltered.groupby('Rating')['Year'].mean().reset_index()\n",
    "rating_year.columns = ['Year', 'Average Rating']\n",
    "\n",
    "sns.lineplot(x=rating_year['Year'], y=rating_year['Average Rating'])\n",
    "plt.title(\"Rating across the years\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Rating\")\n",
    "top_10_genres = df_filltered['Genre'].value_counts(ascending=False).head(10)\n",
    "sns.barplot(x=top_10_genres.values, y=top_10_genres.index,  palette='muted')\n",
    "plt.title('Top 10 Movie Genres')\n",
    "plt.xlabel('Number of Movies')\n",
    "plt.ylabel('Genre')\n",
    "# visualize correlation\n",
    "sns.heatmap(df_filltered[['Rating', 'Votes', 'Duration', 'Year']].corr(), annot=True)\n",
    "sns.pairplot(df_filltered[['Rating', 'Votes', 'Duration', 'Year']])\n",
    "# data preprocessing\n",
    "# encode the categorical features\n",
    "Genre_rated_mean = df_filltered.groupby('Genre')['Rating'].transform('mean')\n",
    "df_filltered['Genre_encoded'] = Genre_rated_mean\n",
    "\n",
    "Director_rated_mean = df_filltered.groupby('Director')['Rating'].transform('mean')\n",
    "df_filltered['Director_encoded'] = Director_rated_mean\n",
    "\n",
    "Name_rated_mean = df_filltered.groupby('Name')['Rating'].transform('mean')\n",
    "df_filltered['Name_encoded'] = Name_rated_mean\n",
    "\n",
    "Act1_rated_mean = df_filltered.groupby('Actor 1')['Rating'].transform('mean')\n",
    "df_filltered['Actor1_encoded'] = Act1_rated_mean\n",
    "\n",
    "Act2_rated_mean = df_filltered.groupby('Actor 2')['Rating'].transform('mean')\n",
    "df_filltered['Actor2_encoded'] = Act2_rated_mean\n",
    "\n",
    "Act3_rated_mean = df_filltered.groupby('Actor 3')['Rating'].transform('mean')\n",
    "df_filltered['Actor3_encoded'] = Act3_rated_mean\n",
    "# split the dataset into train and test\n",
    "X = df_filltered[['Genre_encoded', 'Director_encoded', 'Name_encoded', 'Actor1_encoded', 'Actor2_encoded', 'Actor3_encoded', \"Year\", \"Duration\", \"Votes\"]]\n",
    "y = df_filltered['Rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# print the shape of the train and test sets\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "X_train.fillna(X_train.mean(), inplace=True)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the Lasso model\n",
    "lasso = Lasso()\n",
    "\n",
    "# Train the model\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "rmse = np.sqrt(mse)  # Calculate RMSE\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R2 Score:\", r2)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error,root_mean_squared_error , r2_score\n",
    "\n",
    "# create a Random Forest Regressor\n",
    "RF = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# train the RF\n",
    "RF.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = RF.predict(X_test)\n",
    "\n",
    "# evaluate the RF\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R2 Score:\", r2)\n",
    "\n",
    "Mean Squared Error: 0.08305130946065419\n",
    "Root Mean Squared Error: 0.2881862409287685\n",
    "R2 Score: 0.9572568449619093\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# create a Gradient Boosting Regressor\n",
    "GBR = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# train the GBR\n",
    "GBR.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = GBR.predict(X_test)\n",
    "\n",
    "# evaluate the GBR\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R2 Score:\", r2)\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "# create a AdaBoost Regressor\n",
    "ABR = AdaBoostRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# train the ABR\n",
    "ABR.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = ABR.predict(X_test)\n",
    "\n",
    "# evaluate the ABR\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the unique values in each column\n",
    "for col in df_ratings.select_dtypes(include = \"object\"):\n",
    "    print(f\"The unique values in Column: {col}\")\n",
    "    print(df_ratings[col].unique())\n",
    "#Checking the values in each column\n",
    "for col in df_ratings.select_dtypes(include = \"object\"):\n",
    "    print(f\"the values in Column: {col}\")\n",
    "    print(df_ratings[col].value_counts(), \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of missing values in each column\n",
    "print(df_ratings.isnull().sum())\n",
    "\n",
    "# calculate the threshold for missing values\n",
    "threshold_Na = len(df_ratings) * 0.05\n",
    "print(f\"threshold_Na: {threshold_Na}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the original DataFrame\n",
    "print(\"Original shape:\", df_filtered.shape)\n",
    "\n",
    "# Calculate the threshold for missing values\n",
    "threshold_Na = len(df_filtered) * 0.05\n",
    "print(f\"Threshold for missing values: {threshold_Na}\")\n",
    "\n",
    "# Drop rows where missing values are in columns with less than or equal to threshold_Na missing values\n",
    "for col in df_filtered.columns:\n",
    "    if df_filtered[col].isnull().sum() <= threshold_Na:\n",
    "        df_filtered.dropna(subset=[col], inplace=True)\n",
    "\n",
    "# Print the shape of the filtered DataFrame\n",
    "print(\"Filtered shape:\", df_filtered.shape)\n",
    "\n",
    "# Print the number of missing values in each column of the filtered DataFrame\n",
    "print(df_filtered.isnull().sum())\n",
    "\n",
    "# Print the info of the filtered DataFrame\n",
    "print(df_filtered.info())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
